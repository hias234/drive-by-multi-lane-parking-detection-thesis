\chapter{Testbed - Prototype Implementation}
\label{chap:referenceimplementation}

As already discussed in Chapter \ref{chap:introduction}, sensing a city's parking availability and thus making the parking situation more transparent would be highly beneficial for reducing traffic congestion and greenhouse gas emissions as driver's looking for parking spaces in urban areas could navigate directly to vacant parking spaces close to their destinations. To support this goal, a prototype of drive-by sensing using an optical distance sensor has been designed, implemented, and evaluated. This chapter introduces the prototype and testbed which have been developed to acquire the necessary dataset to run the machine learning experiments. 
%Furthermore, the process to obtain ground truth information is described.

%This chapter will describe the setup and implementation of the prototype. In particular, section \ref{sec:system_design} will discuss the overall envisioned system, the prototype car and the required sensors with their capabilities. In section \ref{sec:experiment_description_data_collection} the experimental setup will be discussed as well as the acquiring of the test dataset through test drives in the city of Linz, Austria. Finally, section \ref{sec:data_processing} will describe the preprocessing of the data, definition of the features and a short description of all used machine learning algorithms with their configurations. 
%\todo{update :)}




%\section{System design}
%\label{sec:system_design}


%\begin{description}
%
%
%\item[Building a test bed] As a first step a test bed has to be built which is able to access the required sensors to record all the necessary data. A Raspberry Pi will be used as processing device because of its popularity and the many compatible sensors which work with this platform. It is connected to a LIDAR-Lite v3 sensor which continuously measures the distance to the nearest obstacle on the right side of the road. A GPS receiver will track the location of the sensing vehicle and a camera will be used to record images of the ground truth for evaluation purposes only. In section \ref{sec:system_design} the complete setup of the test bed is described as well as all the specific hard ware parts and their abilities.
%
%\item[Acquiring a dataset] As soon as the test bed is ready, the sensors should be mounted on the prototype car to be able to start recording the dataset. Test drives should be done in some selected streets in Linz, Austria with the focus of variety of the recorded situations. The test scenes should include single lane as well as multi lane streets and measurements in all streets should be done several times. Furthermore, the car should be driving as it would in regular traffic (not only in the right most lane, etc...) and the scenes should also include high and low traffic scenarios to have a high amount of diverse data. All measured distances, GPS locations and ground truth images have to be saved to files with the according timestamps to be able to evaluate the results of different approaches later on. Furthermore, using the images taken by the camera, ground truth values will be manually labelled in different classes (Parked car, free space, overtaken car, ...). A detailed description about the dataset and the ground truth tagging can be found in section \todo{ref} \ref{sec:dataset}.
%
%\item[Data processing and segmentation] As next step the measured sensor values have to be preprocessed and filtered in order for the following algorithms to work. Sensing and overflow errors as well as outliers in the measurements should be identified and removed before further processing. After the raw data has been filtered, the sensor data has to be segmented. As parking cars and other cases which should be classified consist of several sensor measurements, the corresponding sensor measurements should be grouped together and merged to segments which will be later classified. All preprocessing steps and the segmentation process are described in more detail in section \todo{ref} \ref{sec:data_processing}.
%
%\item[Classification using basic machine learning techniques] Features on the created segments are calculated (for instance length, average distance and variation of the distances) and are being used to train and evaluate several machine learning algorithms which will be compared on their performance. Furthermore, some deep learning models will also be evaluated on the raw sensor data of the segments and will be compared to common machine learning results. The results of all experiments can be found in section \todo{ref} \ref{sec:ml_results}.
%
%\item[Further improvements] \todo{todo}
%
%\end{description}









\section{Used Hardware and Sensor Parts}
\label{sec:test_bed}

Figure \ref{fig:sensing_car} shows the sensing car and its mounted sensors. For collecting, processing, and saving the sensed data a Raspberry Pi 2 Model B\footnote{\url{https://www.raspberrypi.org/products/raspberry-pi-2-model-b/}} is used. A Raspberry Pi was chosen because of its simplicity to connect and access sensors and moreover because it is really easy to program as it is just a regular Linux-based computer. Furthermore, the price of a Raspberry Pi is also quite low (about \euro{35}), therefore the overall cost of the system will remain low.


\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/car.jpg}
	\caption{Prototype of the sensing car, which is composed of a LIDAR Lite v3 optical distance sensor, a GPS receiver, a camera for ground truth collection and a Raspberry Pi as processing device.}
	\label{fig:sensing_car}
\end{figure}


To determine the location of the sensing vehicles while driving through the city, a Navilock USB GPS receiver\footnote{\url{http://www.navilock.de/produkte/G_61840/merkmale.html?setLanguage=en}} is used. The receiver is connected to the Raspberry Pi using a USB port and measures the GPS location (latitude and longitude) at a rate of about 1 Hz. According to the manufacturer the acquired positions are correct in the range of 2.5 meters. The GPS receiver is mounted on the top of the sensing car using a magnet in order to receive more accurate sensor readings. The Navilock USB GPS sensor costs about \euro{50}.

Two major distance sensor technologies can be differentiated: an ultrasonic range sensor and an optical laser distance sensor. While ultrasonic sensors are cheap and widely used in commercial parking assistance systems, they also have limitations in terms of sensing frequency (about 20 Hz) and range (a few meters). These specifications limit the use of ultrasonic sensors for drive-by sensing on multi-lane roads or high speed. Due to these limitations an optical distance sensor, the "Lidar Lite v3\footnote{\url{https://www.sparkfun.com/products/14032}}" is used in our testbed. It measures the time of flight of an emitted laser signal which is reflected by an object in its way. The Lidar Lite v3 can measure distances from a few centimeters up to 40 meters at a frequency of 1 to 500 Hz. Furthermore, it is able to measure distances with an accuracy of about 2.5 centimeters. However, in comparison to ultrasonic sensors, optical sensors are more expensive. While the costs of ultrasonic sensors are often below \euro{10}, the Lidar Lite v3 and other comparable sensors cost about \euro{150}.

The distance sensor is mounted on the co-driver's door using a sucker handle in order to face the right side of the road and to take measures at about 62 cm above the ground. It is connected to the Raspberry Pi via GPIO (General Purpose Input Output) pins, communicating using an I2C (Inter Integrated Circuit) interface, according to the manufacturer's specifications.

Table \ref{table:comparison_us_lidar} shows a comparison of the characteristics of ultrasonic sensors and the Lidar Lite v3 sensor. The most significant limitation is the sampling frequency. While optical laser sensors are based on the speed of light, ultrasonic sensors are based on the speed of sound, which is much lower. Therefore, also the distance between consecutive measurements varies highly from an ultrasonic to an optical laser sensor. At a speed of 50 km/h, the distance between two consecutive measurements is up to 70 cm using an ultrasonic sensor while it is only about 3 cm when using an optical distance sensor.



\begin{table}


\resizebox{\textwidth}{!}{%
\bgroup
\def\arraystretch{1.4}
\begin{tabular}{| r || c | c |}
\hline
   & 
   \textbf{Ultrasonic Range Finder} & 
   \textbf{Lidar Lite v3} \\
%\hline
\hline
  \textbf{Costs} & 
   from about \euro{5,00} to \euro{100,00} &
   about \euro{150,00} \\
\hline
  \textbf{Sampling Frequency} & 
   up to 20 Hz &
   up to 500 Hz \\
   & (at 10 m distance) & \\
\hline
  \textbf{Range} & 
   2 cm - 10 m &
   30 cm - 40 m \\
\hline
  \textbf{Distance between Measurements} & 
   about 70 cm &
   about 3 cm \\
  \textbf{at 50 km/h} & & \\
\hline

\end{tabular}
\egroup
}

\caption{Comparison of ultrasonic sensors and the used Lidar Lite v3 optical distance sensor.}
\label{table:comparison_us_lidar}
\end{table}

A camera is leveraged to capture images of the ground truth, here, a Logitech C922 Pro Stream\footnote{\url{https://www.logitech.com/en-us/product/c922-pro-stream-webcam}} is used. Images with a resolution of 352 x 288 pixels are captured at a frequency of about 30 Hz while the car is sensing parking spaces. The camera is mounted on the co-driver's door on the same horizontal position as the sensor. This way the center of the image will be the position where the distance measurement is taken. The Logitech camera is connected to the Raspberry Pi using an USB port and is available at about \euro{90}.





%\begin{table}
%
%\bgroup
%\def\arraystretch{1.5}
%\begin{tabular}{| r || c | c |}
%\hline
%   & 
%   \textbf{Sampling Frequency} & 
%   \textbf{Costs per Sensor} \\
%%\hline
%\hline
%  \textbf{Lidar Lite v3} & 
%   ~200 measurements/s &
%   \euro{169,00} \\
%\hline
%  \textbf{Navilock USB} & 
%   ~1 measurement/s &
%   \euro{74,90} \\
%   \textbf{GPS receiver} & & \\
%\hline
%
%\end{tabular}
%\egroup
%
%\caption{The used sensors}
%\label{table:sensors_capabilities}
%\end{table}


\section{Collecting Sensor Measurement Data}
\label{sec:sensor_measurement_collection}

A Python script has been developed which saves all sensor readings during the test runs into a text file for later analysis and evaluation. Furthermore, all images captured by the camera are saved into a specific folder on the Raspberry Pi to be able to derive the ground truth information later on (see Section \ref{sec:ground_truth_tagging}).
The sensing of all three devices (GPS, Lidar Lite v3, Logitech camera) is performed asynchronously in separate threads so that none of them influence the other sensor's readings. For accessing the GPS receiver the Python GPS library is used whereas for measuring the distance using the Lidar Lite v3 sensor, an open source library available on github.com is used \footnote{\url{https://github.com/Sanderi44/Lidar-Lite}}. 
%A library called "pygame.camera" was used to capture the images of the webcam. 

Figure \ref{fig:sample_sensor_trace} shows how the sensor data gets collected and furthermore presents a sample of raw sensor data derived during a test drive. The sensor data text file contains GPS and distance measurements. The first column contains the sensor type. The second column gives the time stamp of the Raspberry Pi's system time in seconds. GPS measurements also result in latitude, longitude and speed values while the Lidar Lite measurements result in the measured distance in centimeters.

%\begin{figure}
%	\centering
%	\includegraphics{img/sample-sensor-trace.PNG}
%	\caption{Sample of the collected sensor trace}
%	\label{fig:sample_sensor_trace}
%\end{figure}

\begin{figure}
	\centering
	\includegraphics{img/obtaining-raw-dataset.eps}
	\caption{Acquiring raw sensor data.}
	\label{fig:sample_sensor_trace}
\end{figure}



\section{Raw Data Set}
\label{sec:raw_dataset}

To obtain a dataset, in total 32 test drives were completed in the city of Linz, Austria. The street scenarios have been selected aiming at a high variability of road scenarios. The test scenes include single lane as well as multi lane streets. While collecting the test measurements the sensing car was driving as it would in regular traffic (not only in the right most lane, etc.) and the scenes also include high and low traffic scenarios. For all street scenarios, measurements have been repeated multiple times.
Figure \ref{fig:gps_locations_dataset} shows an aggregated GPS trace of all test drives on a map of Linz. Red dots represent parking cars while black dots represent other objects. In total \textbf{444.427 distance measurements} and \textbf{14.997 GPS measurements} were taken, which makes up about 15,9 megabytes of distance and GPS data. 
Additionally, \textbf{266.309 images} were captured to record the ground truth of the sensed scenes. The images, which make up about \textbf{5.072 megabytes}, are used to determine the ground truth of the recorded scenes. Sensed data and ground truth data are applied to supervised machine learning algorithms later on. Ground truth tagging is described in the following section.



\begin{figure}
	\centering
	\includegraphics{img/gps_data_recorded_data_parking_spaces.PNG}
	\caption{GPS trace of all recorded samples in the acquired dataset, while 32 test drives in Linz, Austria. Red dots represent parking cars.}
	\label{fig:gps_locations_dataset}
\end{figure}


















\chapter{Data Processing and Machine Learning Approaches}
\label{chap:data_processing_and_ml}

This chapter describes all the steps which are necessary to prepare the raw dataset for the use in machine learning. The required steps include manual ground truth tagging (Section \ref{sec:ground_truth_tagging}), data preprocessing (Section \ref{sec:data_preprocessing}), and data segmentation (Section \ref{sec:data_segmentation}). 
Furthermore, the creation of the features for the dataset used in the machine learning experiments is described (Section \ref{sec:feature_calculation}) as well as the machine learning experiments including the experiment methods (Section \ref{sec:machine_learning_experiments}).
Finally, the use of parking space maps to improve classification results and acquiring these parking space maps is described in Section \ref{sec:parking_space_maps}.





\section{Ground Truth Tagging}
\label{sec:ground_truth_tagging}

The recorded raw dataset contains distance and position measurements as well as images showing the ground truth at a specific point in time. To be able to process the ground truth information included in the images, a manual ground truth tagging task has to be performed. Figure \ref{fig:ground_truth_tagging_ui} shows the user interface of the program which has been implemented to manually tag an image with one of nine pre selected class labels. The file name of each image is the time stamp of the recording. After the images are loaded, the user has to tag the situation which is present at the center of the image (red line). For instance, Figure \ref{fig:ground_truth_tagging_ui} shows a parking car at the center of the image. In addition, on the right side of the image, a car which has been overtaken by the sensing vehicle is also visible, but as it is not in the center, the image is labelled as "parking car".
Possible class labels are: 

\begin{figure}
	\centering
	\includegraphics[width=0.85\textwidth]{img/ground_truth_tagging_ui.PNG}
	\caption{User interface for manual tagging of parking situations. \todo{crop}}
	\label{fig:ground_truth_tagging_ui}
\end{figure}

\begin{description}

\item[Parking car:] A parking car is a non-moving vehicle which is parking at the right side of the road. There are three kinds of parking vehicles: \textbf{\textit{Parallel parking cars}}, \textbf{\textit{perpendicular parking cars}}, and \textbf{\textit{angular parking cars}}. Figure \ref{fig:types_of_parking_cars} shows the different types of parking spaces.

\item[Overtaking situation:] This class label means that the sensing vehicle is overtaking another road vehicle. The user can choose between \textbf{\textit{overtaken cars}}, \textbf{\textit{overtaken motorcycles}} and \textbf{\textit{overtaken bicycles}}.

\item[Other parking vehicle:] The user can also tag \textbf{\textit{parking motorcycles}} and \textbf{\textit{parking bicycles}}. This class represents obstacles which can be located on parking spaces and therefore make it unavailable for parking.

\item[Free space:] This class label should be used, when none of the above listed class labels are applicable. It represents a free space where it is maybe possible to park a vehicle. Yet only with the help of a parking space map, a free space can be marked as vacant parking space as a random free space may be an illegal parking space (e.g. a driveway).

\end{description}



The output of the ground truth tagging process is persistently saved as a text file where each line contains an entry with a time stamp and its corresponding tagged ground truth class label. This ground truth file will be further processed during the machine learning process as described in \todo{ref}.


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/types-of-parking-cars.eps}
	\caption{Different types of parking cars.}
	\label{fig:types_of_parking_cars}
\end{figure}


\section{Data Preprocessing}
\label{sec:data_preprocessing}

Before the raw dataset can be used to create the input features for machine learning models, a few preprocessing steps have to be taken. First of all, error values of the GPS sensor and the LiDAR Lite v3 sensor are deleted. The GPS sensor usually shows erroneous output (NaN values) when it starts sensing due to the fact that it is not yet connected to enough satellites. The distance sensor can provide overflow measurements when the target object is too far away or too close to the sensor. In both cases it will give an output of less than 10 centimeters. Furthermore, outliers where a single measurement differs greatly (more than 1 meter) from the enclosing measurements are also detected so that they won't cause over-segmentation in the data segmentation process (discussed in Section \ref{sec:data_segmentation}). All of the detected error/overflow/outlier measures will be simply deleted from the raw dataset before further processing to ensure the quality of the sensor measurements.

Another necessary preprocessing step is the filtering of standing situations of the car. For instance, when the sensing vehicle is waiting at a traffic light or in a traffic jam, such situations would not lead to meaningful samples for the machine learning process and would decrease the accuracy of the classification results. As the sensing vehicle stands still it measures the distance always to the same object on the right side for a long time and this way the features which are calculated as input for the machine learning process (described in Section \ref{sec:feature_calculation}) would may be misleading. Therefore, all situations where the car is driving at a speed lower than 1 m/s are deleted from the dataset.


\subsection{Sensor Measurement Interpolation}
\label{sec:sensor_measurement_interpolation}

Due to the fact that all used sensors measure at different frequencies, the sensor data of all sensors has to be unified before the next steps can be taken. The distance sensor measures at a much higher frequency (about 100 Hz) than both the GPS sensor (about 1 Hz) and the camera (about 30 Hz). Therefore, the GPS location as well as the ground truth at each distance measurement have to be approximated. Linear interpolation using the timestamp of the measurements is being used to calculate the approximate location and ground truth at the times of all distance measurements. Figure \ref{fig:preprocessing_dataset} shows how the raw sensor data and the ground truth data get unified so that a list of data points containing distance, GPS position and ground truth can be derived.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/dataset-preprocessing.eps}
	\caption{All sensor measurements are being merged to obtain a dataset where all samples are containing time, distance, location and ground truth information.}
	\label{fig:preprocessing_dataset}
\end{figure}





\section{Data Segmentation}
\label{sec:data_segmentation}

As the used distance sensor measures at a high frequency, it usually collects several distance measurements belonging to the same object. For example, if the sensing vehicle passes a parallel parked car (having a length of 4 meters) at a speed of 30 km/h it will collect approximately 24 measurements belonging to the passed car. To be able to automatically evaluate the performance of different approaches to sense parking cars and other objects the sensor measurements belonging to the same objects have to be grouped together. After the raw data points, which contain all information, are being derived by the preprocessing step, the segmentation process has to be performed, which groups similar consecutive data points together.

There are several strategies how sensor data can be segmented, for example using sliding windows. However, such approaches will not work accurate enough in the case of street side parking detection, as the objects which should be segmented range from a few centimeters to a length of tens of meters. Therefore, a new strategy to segment the sensor data points is proposed. The used segmentation algorithm searches for high frequent changes in the distance signal and starts a new segment everywhere where the distance difference between two consecutive measurements is above a certain segmentation threshold. Furthermore, if the timestamp of two measurements differ more than one second the start of a new segment will also be detected (this can be the case if the sensing vehicle was standing or driving at a very low speed and many measurements have been deleted in the preprocessing steps). 

The best threshold to detect the segmentation points in the distance sensor signal has been identified by running several experiments with different thresholds and by manually reviewing the resulting segments for under- and over-segmentation. The resulting segmentation threshold turned out to be $1,05$ meters. This means that if two consecutive measures differ more than $1,05$ meters, the algorithm will assume that the sensor measurement belongs to a new object (e.g. a parking car or an overtaken car) and will detect a new segment.

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{img/segmentation_example.PNG}
	\caption{Result of the segmentation algorithm. Three parking cars (green dots and orange lines) and two free space segments (yellow dots and black lines) have been detected.}
	\label{fig:segmentation}
\end{figure}

Figure \ref{fig:segmentation} shows an example of the segmentation process. All points represent distance measurements. The green points represent measurements belonging to a parking car whereas the yellow points show free space measurements. The lines represent detected segments which belong to the same object classes. Black lines represent free space and orange lines represent a parking car. Thus, the segmentation shown in Figure \ref{fig:segmentation} has successfully detected three parking car- and two free space segments. 

Another challenge of the segmentation task is to estimate the ground truth classes of the segments. So far, only each sensor reading got a ground truth class assigned. However, as described earlier, each segments consists of multiple sensor measurements and therefore it is possible that it also consists of multiple different ground truth classes. You can see this phenomenon in Figure \ref{fig:segmentation} where both "Free space"-segments also have some measurements assigned which are marked as "Parking Car". To overcome this problem, the segments are tagged with the label which is tagged in a majority of the sensor readings. This approach turned out to work well as in most cases only the first and last ground truth values are assigned wrong. 




\section{Feature Calculation}
\label{sec:feature_calculation}

To conduct machine learning experiments with common machine learning models, feature values for each segment have to be computed using the sensor measurements. These feature values serve as input values of the machine learning model to help classifying the corresponding segment to one of the available class labels. All computed features which were used as input are:

\begin{description}

\item[Average distance to the sensing vehicle:] The average of the segment's distance sensor measurements to the closest object on the right side of the road in meters.

\item[Length:] The distance the sensing vehicle drove during sensing the segment in meters (the distance between the first and last GPS location).

\item[Duration:] The duration in seconds the sensing vehicle needed to sense the segment (the last time stamp - the first time stamp).

\item[Number of distance measurements:] The number of distance measurements assigned to the segment.

\item[Variance of the distance measurements:] The variance of all distance measurements belonging to the segment.

\item[Speed:] The speed of the sensing vehicle while sensing the segment in m/s.

\item[Acceleration:] The acceleration of the sensing vehicle while sensing the segment in $m/s^2$.

\item[Distance difference to the next/previous segment:] Difference of the average distance measurements to the next/previous segment in meters.

\end{description}


\subsection{Feature Analysis}

The goal of each feature is to help identify the true class of a sample which should be classified. The feature analysis presented in this section should determine if all features are necessary and beneficial to this goal. Too many features maybe do not produce better results and also increase the learning time of all models. Using the tool Weka\footnote{\url{https://www.cs.waikato.ac.nz/ml/weka/}} two analytic approaches have been used, which show for each feature how useful it is for the classification tasks:

\begin{description}

\item[Information Gain Analysis:] Weka provides a tool called "InfoGainAttributeEval" which calculates the information gain (also known as entropy) of all the features for the classification task. The features which will contribute more to an accurate classification result will have a higher information gain than others. The resulting values are in the range of 0 to 1.

\item[Learner based Feature Analysis:] This approach tests several feature subsets on a real machine learning algorithm (a J48 decision tree is used) and identifies the best feature subset. The process is repeated with different training sets and counts how often the features are used in the different runs. If a feature is used in all runs it gets a value of 1 whereas if it is never used its value will be 0. So the higher the value is, the more the feature helps in the classification task.

\end{description}

Table \ref{table:feature_analysis} shows the results of the feature analysis process. Both results are showing that there are five important features, which provide the most value to classification (average distance, distance difference to next and previous segment, length and number of measurements). All of them have a high information gain and are used in all feature subsets which lead to the best results in the learning based analysis. The other four features (duration, distance variance, average speed and average acceleration) are only used in a minority of the feature subsets and also have lower information gain. This means that they won't contribute so much to a good classification result. However, these findings have to be confirmed by machine learning experiments later on.



\begin{table}


%\resizebox{\textwidth}{!}{%
\centering
\bgroup
\def\arraystretch{1.4}
\begin{tabular}{| r || c | c |}
\hline
   & 
   \textbf{Information Gain} & 
   \textbf{Learning based Score} \\
\hline
  \textbf{Average distance} & 
   0.361 &
   1.0 \\
\hline
\textbf{Diff to next} & 
   0.232 &
   1.0 \\
\hline
\textbf{Diff to prev.} & 
   0.233 &
   1.0 \\
\hline
\textbf{Length} & 
   0.223 &
   1.0 \\
\hline
\textbf{Nr of measures} & 
   0.178 &
   1.0 \\
\hline
\textbf{Duration} & 
   0.175 &
   0.2 \\
\hline
\textbf{Distance variance} & 
   0.055 &
   0.0 \\
\hline
\textbf{Avg speed} & 
   0.019 &
   0.3 \\
\hline
\textbf{Avg acceleration} & 
   0.011 &
   0.1 \\
\hline


\end{tabular}
\egroup
%}

\caption{Feature analysis showing the information gain and learning based score of all calculated features.}
\label{table:feature_analysis}
\end{table}






%Maybe 2-3 Graphics of the most important features (from Weka)









\section{Derived Dataset}
\label{sec:derived_dataset}

After data preprocessing, data segmentation and feature calculation have been performed, the final dataset which is being used for the further machine learning is available. The dataset is made up of samples with all feature values described in the previous section and the corresponding class label. The dataset consists in total of \textbf{14.303 samples} which are composed of \textbf{11.811 free spaces}, \textbf{2.216 parking cars}, \textbf{207 overtaking situations} and \textbf{69 other parking vehicles}. 
Obviously, the dataset is highly imbalanced. About 82.6\% of all samples are tagged as "free spaces" while only about 1.4\% are tagged as "overtaking situations" and 0.5\% as "other parking vehicles". 






\section{Machine Learning Experiments}
\label{sec:machine_learning_experiments}

There are a lot of different machine learning approaches, which can be chosen from. All of them have their strengths and weaknesses and are applied in different scenarios. To find out which machine learning model works best, experiments are being performed and the results are compared so that the best model can be identified. 
The overall goal of the experiments is to find the feature subset and the machine learning model which in combination produce the highest accuracy in classifying all segments. 





\subsection{Tested Machine Learning Models}
\label{sec:machine_learning_models}

This section will discuss all machine learning models which were investigated on their performance. Furthermore, also all parameters which could be set for the different machine learning models will be presented. The tested machine learning models are:

\todo{passt hier eine kurze Beschreibung von den jeweiligen getesteten Algorithmen? Soll ich da auch immer grundlegende Papers dazu zitieren?}

\paragraph{Naive Bayes Classifier}
\paragraph{Decision Trees}
\paragraph{Random Forest Classifier}
\paragraph{Neural Networks}
\paragraph{kNN Classifier}
\paragraph{Support Vector Machines}
\paragraph{Ensemble Methods}


\subsection{Evaluation and Used Tools and Frameworks}

To be able to get reliable results for the performance of the different classifiers it is important to have a comparable evaluation method. The used method in this thesis is 10-fold cross validation. Using this technique, all samples of the dataset are split into 10 equally sized parts (folds). Then 1 fold is selected as test data while all the other folds are used for training the classifier. This process gets repeated 10 times, so that every fold has been used as test data exactly once. Cross validation ensures, that there is no sample in the test data which has been previously learned by the classifier in the training process. Moreover, it is important that every sample gets predicted once, so that the performance measures are realistic.

Another process which helps to get better performance is to shuffle the dataset, before it is divided into folds for cross validation. The shuffling ensures that it is likely that only one of two consecutive segments is in the test data, while the other one is in the training data. This helps to have more diverse training data as consecutive segments usually are more alike than others. For example, usually perpendicular parking spaces are in the same area. Therefore, if the dataset gets shuffled the ratio between perpendicular parking cars will be approximately the same for the test- and training data.


%\subsection{Used Tools and Frameworks}

To conduct the machine learning experiments the dataset is processed using the Python programming language. Furthermore, the classifiers of the scikit learn machine learning framework are used for the experiments. Scikit learn is a framework for Python which offers all of the above discussed classifiers as well as tools to support the evaluation process. 






\section{Deep Learning}
\label{sec:deep_learning}

Deep learning gained a lot of attention in the last few years because of its raising classification accuracy results. For example, deep learning models became the first classifiers which could compete with humans on simple classification tasks. A deep learning network is a special form of a neural network with a possibly large amount of hidden layers between the input layer and the output layer. Furthermore, in most applications raw sensor measurements are used as input instead of pre-computed feature values (which are derived from the raw sensor values). In theory, a simple neural network containing only one hidden layer can derive any function given enough hidden elements and long enough training time. However, that does not mean that such a simple neural network will find a optimal solution and that it will find it in a finite amount of time. Adding more hidden layers (to get a deep neural network) can help to find a solution more easily and with less training time. 
Figure \ref{fig:densely_dl_network} shows an example of a deep learning network with several hidden layers which are densely-connected.

Due to the classification power, deep learning should also be tested for the use in classifying parking situations using our dataset. It should be investigated if deep learning can be used to do this kind of classification on raw sensor data and the results should be compared to classical machine learning algorithms (described in Section \ref{sec:machine_learning_models}) using pre-computed feature values.

%\subsection{Dataset for Deep Learning Experiments}

As input for the deep learning experiments a dataset containing all sensor measurements for each segment is needed. In a pre-processing step a dataset for the deep learning experiments is created which is made of a vector of sensor measurements for each segment. Each vector has a fixed size of 1024 sensor measurements (that will be enough for most segments). In most cases the segments have a much lower number of measurements. In these cases the measurements will be zero padded to the required size. If a segment has more than 1024 measurements, only the first 1024 will be used. This step is necessary to have a fixed length input for the experiments as the input size for all segments has to be equal. A vector represents the input layer of the neural network. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/deep_learning_basic_model_2.eps}
	\caption{A basic deep learning network having sensor values as input and the probability of all classes as output values. All layers are densely connected with each other.}
	\label{fig:densely_dl_network}
\end{figure}





\subsection{Tested Deep Learning Models}

As baseline deep learning network a dense neural network with 5 hidden layers and 64 hidden elements per layer is used. This basic deep learning network should show how a densely-connected deep learning model performs on the raw sensor dataset. Figure \ref{fig:densely_dl_network} shows the illustration of the created deep learning network containing of an input layer with 1024 elements (one for each sensor value), 5 hidden layers containing 64 hidden elements per layer and four output elements which calculate the probability of each class for the given input. The class which gets predicted is the one with the greatest probability out of the four classes. 

In addition to the basic deep neural network, several known strategies should be tested to improve the performance of the deep learning approach. First of all, dropout between the layers should be tested. Dropout is a technique in deep learning to handle over-fitting of the neural network. If dropout with a prability $p$ is introduced between two layers, a connection between two elements is maintained only at a probability of $1-p$. The result is a more sparsely connected deep neural network which should be less prone to over-fitting. Dropout should be tested between several layers and with different values for the deletion-probability $p$.

Another popular technique to improve deep learning models is the use of convolution. Convolutional neural networks are especially popular for image classification. However, convolution can also be applied on the one-dimensional distance sensor data available in our dataset. Different configurations for the configuration of such a convolutional neural network should be tested and the performance should be compared to the other approaches.

%\subsection{Tools for DL Development}

To implement the deep learning models, the programming language Python\footnote{\url{https://www.python.org/}} (version 3.6.4) is used as well as the TensorFlow framework\footnote{\url{https://www.tensorflow.org/}}. TensorFlow is built for creating neural network models and especially for building deep learning models using a lot of pre-built components with a easy readable syntax. 




\section{Parking Space Maps}
\label{sec:parking_space_maps}

When sensing a city's parking availability situation parking space maps are of great importance. As already discussed the machine learning models only classify road segments to "free spaces" (rather than "vacant parking spaces") which means that there are no parking cars and a car can possibly park there. However, parking space maps are necessary as it cannot be sensed if it is legal to park at a certain free space. For example, a free space might just be a driveway or a bus station. Vacant parking spaces can be identified when their location is compared to a parking space map matching parking spaces there. 

Furthermore, the use or parking space maps will also possibly improve the performance of the classification task. If only segments in areas where parking spaces are located are used as training set, the overall data will be much more balanced (there will be less "free space" segments, but still the same amount of parking cars) and moreover the machine learning model will only be trained with relevant samples as all samples which are not close to parking spaces won't be classified later on anyway.



\subsection{Parking Space Map of Linz, Austria}

As it is crucial to have accurate parking space maps, the goal was to obtain such maps for the area of the experiment. As a first try, the APIs of OpenStreetMap\footnote{\url{https://www.openstreetmap.org}} have been used to obtain map data of the city of Linz, Austria. The maps of OpenStreetMap are created and maintained by independent communities which operate in their free time. Parking spaces are available in the downloaded map data but unfortunately, only parking spaces of big parking lots (of shopping malls, etc.) are available but most of the city's road side parking spaces, which are being sensed in this work are not present. Therefore, the parking space maps of OpenStreetMap cannot be used due to insufficient data.

Another possibility to obtain a parking space map is to use data from city authorities in Linz. City authorities keep detailed digital maps of the streets as well as the road side parking spaces. However, the only format available are several DXF- or DWG-files, created using the program AutoCAD\footnote{\url{https://www.autodesk.eu/products/autocad/overview}}, which is commonly used for designing and drawing as well as for geographical applications. There exist libraries which can read all components of DXF-files and their attributes but unfortunately there are no geographical position data attached to the components and therefore the GPS location of all parking spaces is unknown. Only an absolute coordinate from the drawing is available, but cannot be converted to a GPS location. This fact makes it impossible to use the parking space map in the park sensing prototype as the parking spaces of the map cannot be matched with the sensed segments obtained during the experiments.



\subsection{Approximating Parking Space Maps}

As parking space maps are unavailable in a sufficient quality from outside sources, an algorithm is introduced to derive a coarse parking space map from the dataset which was acquired during the test drives. The dataset contains the GPS positions of over 2.000 parking cars and most streets have been sensed several times. As of the limited amount of data, the goal is to create a coarse parking space map which contains areas where it is likely that vehicles park (parking zones). 

The proposed algorithm uses clustering to group parked cars to parking zones. The idea is that if there are several cars parking at the same location in several test drives then it is likely that there are parking spaces at that position. A DBSCAN\footnote{Density-Based Spatial Clustering of Applications with Noise - \url{http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html}} clustering algorithm (part of the scikit learn\footnote{\url{http://scikit-learn.org/}} python framework for machine learning) is being used to cluster the position of the parked cars. The DBSCAN algorithm searches for high density core samples and expands clusters from them. It takes the a similarity function which compares two data points and a minimum similarity as input parameters.

As similarity measure for the clustering algorithm a custom function has been implemented. Two parked cars are considered to be similar if the sensing car is driving in the same direction while sensing and if both cars are close to each other in terms of their GPS position. The check of the direction the sensing car is going while sensing is performed ensure that both compared parking cars are on the same side of the street. If the sensing vehicle is going in the opposite direction while sensing another parked car both cars will be at the same position but at the opposite side of the street. If the first check is passed, the similarity function will compute the distance in meters between the GPS positions of both parked cars which are compared. The described similarity function is used by the DBSCAN clustering algorithm to identify core samples and to cluster multiple parking cars to parking zones. The maximum distance between two cars to be considered to be similar is set to eight meters. If two parking cars are further apart they are not considered to be able to be in the same parking zone.

%\begin{figure}
%
%\begin{tabular}{ c c }
%	\includegraphics[width=0.48\textwidth]{img/parking_space_map_all_gps.PNG}
%    & 
%	\includegraphics[width=0.48\textwidth]{img/parking_space_map_zones.PNG}
%    \\
%\end{tabular}
%
%	\caption{Parking space map.}
%	\label{fig:parking_space_map}
%\end{figure}

\begin{figure}
\centering
\def\arraystretch{1.2}

\begin{tabular}{ c }
	\includegraphics[width=0.9\textwidth]{img/parking_space_map_all_gps_2.PNG}
	\\
	(a) All GPS points
	\\
    \\
	\includegraphics[width=0.9\textwidth]{img/parking_space_map_zones_2.PNG}
    \\
    (b) Detected parking zones
\end{tabular}

	\caption{Approximation of parking zones from the dataset. (a) is showing all GPS points at the selected region while (b) is showing the derived parking zones (clusters of parked cars).}
	\label{fig:parking_space_map}
\end{figure}

Figure \ref{fig:parking_space_map} shows two images of the same test region in Linz, Austria. In (a) there are shown the GPS positions of all sensed segments which are part of the dataset, while in (b) only the GPS positions of segments which are in parking zones are shown. In both pictures green dots represent parking cars, while black dots show other segments. The semitransparent boxes which are shown in (b) show the clusters (parking zones) which were derived by the proposed algorithm. 
Only areas where there are lots of parking cars are extracted to be parking zones. In total 131 parking zones are derived from the whole dataset containing 97.2\% of all parking cars in the dataset. The remaining 62 parking cars are classified as noise by the clustering algorithm because there are no other parking cars in the dataset nearby. A bigger dataset with more test drives during different times would be needed to get a more accurate parking space map and to reduce the parking cars which are classified as noise.




\subsection{Using Parking Space Maps to improve Classification Results}

As already mentioned, it does not make sense to classify segments which are not close to parking zones. Such segments will not make a difference to the end result as it is known that at these positions parking is not possible (or legal) and thus the classification result will not make any difference. 
To improve the classification results, only segments which are close to parking zones will used for training and for evaluating the machine learning models. The dataset, which is described in Section \ref{sec:derived_dataset}, is filtered by matching the GPS positions of the segments with the bounding boxes of the parking zones. All segments which are contained in at least one bounding box of a parking zone will remain in the dataset while all others are deleted.

The remaining filtered dataset is much smaller than the original one. In total it consists of 8.591 segments, which are composed of 6.345 free spaces, 2.131 parking cars, 55 overtaking situations and 60 other parking vehicles. This filtered dataset is much more balanced than the original one, because almost all of the parking cars are still included while approximately half of the free spaces, which has been the dominating class, is deleted. Usually the more balanced dataset will lead to better values of precision and recall of the parking car class, but this has to be confirmed in the machine learning experiments. The results of the classification task using the parking space map filtered dataset can be found in Section \todo{ref}.





